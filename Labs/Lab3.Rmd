---
title: "PSTAT 126"
subtitle: "Lab 3"
date: "08/21/2023"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Multiple Linear Regression (MLR)

We can extend the Simple Linear Regression to a Multiple Linear Regression by incorporating more that one predictor:
$$y_i=\beta_0+\beta_1x_{i1}+ \ldots + \beta_px_{ip} +\epsilon_i, \qquad i=1,\ldots,n$$
Using matrix notation, the model can be written as:
$$\mathbf{y}=\mathbf{X}\boldsymbol \beta+\boldsymbol\epsilon, \qquad \boldsymbol \epsilon \sim N_n(\boldsymbol 0,\sigma^2\boldsymbol I_n)$$
To get the LS solution of $\boldsymbol \beta$ we minimize the Sum of Squared Residuals:

$$SSR=(\mathbf{y}-\mathbf{X}\boldsymbol \beta)^T(\mathbf{y}-\mathbf{X}\boldsymbol \beta)$$
We obtain the solution: $\hat{\boldsymbol {\beta}}_{LSE} = {(\boldsymbol X^T\boldsymbol X)^{-1}\boldsymbol X^T \boldsymbol y}$. Additionally, it can be proved that $\hat{\boldsymbol\beta} \sim N_{p^*}(\boldsymbol \beta,{(\boldsymbol X^T\boldsymbol X)^{-1}}\sigma^2 )$.\
\
Therefore, it is possible to obtain the Standard Error of $\hat\beta_j$:

$$SE(\hat\beta_j)= \sqrt{\hat\sigma^2\left[X^TX\right]^{-1}_{jj}}$$
Where $\hat\sigma^2=\frac{(\mathbf{y}-\mathbf{X}\hat{\boldsymbol \beta})^T(\mathbf{y}-\mathbf{X}\hat{\boldsymbol \beta})}{n-p^*}$, with $p^*=p+1$.

## Data Example
```{r sum2, echo = TRUE}
library(faraway)
data(diabetes)
model <- lm(weight~chol+ stab.glu+ hdl+height+waist+age, data=diabetes)
summary(model)
```

#### Inference

```{r sum3, echo = TRUE}
summary(model)$coefficients
```

We get $\hat{\boldsymbol {\beta}}$:

```{r}
library(faraway)
betas = summary(model)$coefficients[,1]
betas
coef(model) # this returns the same results
```

We can get $SE(\hat\beta_j) \qquad j=0,\ldots,p$:
```{r}
summary(model)$coefficients[,2] 
```

Coefficient od determination $R^2$:
```{r}
summary(model)$r.squared
```

Residuals:
```{r}
Res=residuals(model) 
```


Standard deviation of residuals ($\hat\sigma$):
```{r}
sqrt(sum(Res^2) /model$df.residual) # using formula
sigma(model) # using R built-in function
```

Confidence and Prediction Intervals:
```{r}
# LSE of coefficients with CI
confint(model, level = .95)
# 95% CI for the mean response w/ Age=34,chol=186,gluc=85,hdl=46,height=66,waist=46
new = data.frame(chol=186, stab.glu=85, hdl=46,height=66,waist=46,age=34)
ans1 = predict(model, new, se.fit = TRUE, interval = "confidence", level = 0.95, type = "response")
ans1$fit
# 95% PI for a new observation w/ Age=34,chol=186,gluc=85,hdl=46,height=66,waist=46
ans2 = predict(model, new, se.fit = TRUE, interval = "prediction", level = 0.95, type = "response")
ans2$fit
```





## Normal Distribution Review

##### Let $X_1, X_2,...,X_n \stackrel{iid}{\sim} N(\mu, \sigma^2)$. Then the pdf is given for any $\mu\in \mathbb{R},\,x\in \mathbb{R},\,\sigma>0$

Then define the following: $E(X_i)=\mu, Var(X_i)=\sigma^2\,\forall i$

##### The CDF is given by, $F_X(x)=P(X \le x)= 1-P(X > x)$
For $\mu=0, \sigma=1$, we have the standard normal $Z \sim N(0,1)$:

Some properties are as follows:

1) $P(Z< z)= P(Z> -z)$ 
2) $P(|Z| > z)= 2P(Z>z)= 2P(Z<-z)$
3) if $P(Z> z_{\alpha})=\alpha$, then 

$P(z_{1-\alpha/2}< Z < z_{\alpha/2})$

$=P(-z_{\alpha/2}< Z < z_{\alpha/2})=1-\alpha$

##### Computation:

```{r}
# consider the x-values
x<- seq(-3.5, 3.5, length.out=100)

# pdf of N(0,1)
f<- dnorm(x)

f2<- dnorm(x, mean =0, sd = .5) # pdf of N(0,.5)
f3<- dnorm(x, mean = 1, sd = .75) # pdf of N(1,.75)
f4<- dnorm(x, mean= -2, sd = 1.5) # pdf of N(-2,1.5)

# plot of pdfs
plot(x, f, type = "l", lwd=1, col= 1, ylab="Density", ylim=c(0, .95))
lines(x, f2, type = "l", lwd=1, col= 2)
lines(x, f3, type = "l", lwd=1, col= 3)
lines(x, f4, type = "l", lwd=1, col= 4)
legend(2, .8, legend=c("N(0,1)", "N(0,0.5)","N(1,0.75)","N(-2,1.5)"),
       col=c(1,2,3,4), lty=1, cex=0.8)
```

```{r}
#cdf
cf1<- pnorm(x) # pdf of N(0,1)
cf2<- pnorm(x, mean =0, sd = .5) # pdf of N(0,.5)
cf3<- pnorm(x, mean = 1, sd = .75) # pdf of N(1,.75)
cf4<- pnorm(x, mean= -2, sd = 1.5) # pdf of N(-2,1.5)

#quantiles 
q<- seq(0,1, length.out=100)
qf1<- qnorm(q) # pdf of N(0,1)
qf2<- qnorm(q, mean =0, sd = .5) # pdf of N(0,.5)
qf3<- qnorm(q, mean = 1, sd = .75) # pdf of N(1,.75)
qf4<- qnorm(q, mean= -2, sd = 1.5) # pdf of N(-2,1.5)

par(mfrow= c(1,2))
# plot of cdfs
plot(x, cf1, type = "l", lwd=1, col= 1, ylab="CDF", ylim=c(0, 1))
lines(x, cf2, type = "l", lwd=1, col= 2)
lines(x, cf3, type = "l", lwd=1, col= 3)
lines(x, cf4, type = "l", lwd=1, col= 4)
legend(-3.6, 1.02, legend=c("N(0,1)", "N(0,0.5)","N(1,0.75)","N(-2,1.5)"),
       col=c(1,2,3,4), lty=1, cex=0.6)

# plot of quantiles
plot(q, qf1, type = "l", lwd=1, col= 1, ylab="Quantile function", ylim=c(-3,3))
lines(q, qf2, type = "l", lwd=1, col= 2)
lines(q, qf3, type = "l", lwd=1, col= 3)
lines(q, qf4, type = "l", lwd=1, col= 4)
legend(0, 3, legend=c("N(0,1)", "N(0,0.5)","N(1,0.75)","N(-2,1.5)"),
       col=c(1,2,3,4), lty=1, cex=0.6)

```

##### Simulation of a linear regression

Consider the x covariate in $[0,1]$ distributed uniformly.
Then we have the model $$Y_i=1.21+ 2.445X_i + \epsilon_i, i=1,2,...,200$$

```{r}
# generate x's
set.seed(54321)
x<- runif(200)

# generate y's
y<- 1.21+ 2.445*x+ rnorm(200)

# fit linear model
fit1<- lm(y~x)

# plot the regression
plot(x,y, col="red", pch=19)
abline(fit1)

fit1$coefficients






